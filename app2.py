import streamlit as st

import openai

import fitz Â # PyMuPDF for PDFs

import docx

import os

import hashlib

import logging

from sentence_transformers import SentenceTransformer, util



# Configure logging

logging.basicConfig(level=logging.INFO, filename='api_calls.log',

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  format='%(asctime)s - %(levelname)s - %(message)s')



# Set OpenAI API Key (Replace with your key or use secrets)
 openai.api_key = st.secrets["OPENAI_API_KEY"]

# Load the embedding model

embedding_model = SentenceTransformer("all-MiniLM-L6-v2")



# Function to extract text from PDFs

def extract_text_from_pdf(file):

Â  Â  try:

Â  Â  Â  Â  doc = fitz.open(stream=bytes(file.read()), filetype="pdf")

Â  Â  Â  Â  return "\n".join([page.get_text("text") for page in doc])

Â  Â  except Exception as e:

Â  Â  Â  Â  logging.error(f"Error extracting PDF text: {e}")

Â  Â  Â  Â  return ""



# Function to extract text from DOCX

def extract_text_from_docx(file):

Â  Â  try:

Â  Â  Â  Â  doc = docx.Document(file)

Â  Â  Â  Â  return "\n".join([para.text for para in doc.paragraphs])

Â  Â  except Exception as e:

Â  Â  Â  Â  logging.error(f"Error extracting DOCX text: {e}")

Â  Â  Â  Â  return ""



# Function to extract text from TXT

def extract_text_from_txt(file):

Â  Â  try:

Â  Â  Â  Â  return file.read().decode("utf-8")

Â  Â  except Exception as e:

Â  Â  Â  Â  logging.error(f"Error extracting TXT text: {e}")

Â  Â  Â  Â  return ""



# Function to get OpenAI response

def ask_chatgpt(question, document_text):

Â  Â  logging.info(f"Received question: {question}")

Â  Â  cache_key = hashlib.sha256((question + document_text).encode()).hexdigest()

Â  Â  if cache_key in st.session_state["response_cache"]:

Â  Â  Â  Â  logging.info("Answer retrieved from cache.")

Â  Â  Â  Â  return st.session_state["response_cache"][cache_key]

Â  Â 

Â  Â  logging.info("Calling OpenAI API.")

Â  Â  try:

Â  Â  Â  Â  response = openai.ChatCompletion.create(

Â  Â  Â  Â  Â  Â  model="gpt-4",

Â  Â  Â  Â  Â  Â  messages=[{"role": "system", "content": "You are an assistant."},

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {"role": "user", "content": f"{document_text}\n\n{question}"}]

Â  Â  Â  Â  )

Â  Â  Â  Â  answer = response["choices"][0]["message"]["content"]

Â  Â  Â  Â  st.session_state["response_cache"][cache_key] = answer Â # Cache response

Â  Â  Â  Â  return answer

Â  Â  except Exception as e:

Â  Â  Â  Â  logging.error(f"Error calling OpenAI API: {e}")

Â  Â  Â  Â  return "Error in retrieving the answer. Please try again."



# Streamlit UI

st.title("ðŸ“„ Multi-File Document Chatbot")



# Initialize session state

if "documents" not in st.session_state:

Â  Â  st.session_state["documents"] = ""

if "response_cache" not in st.session_state:

Â  Â  st.session_state["response_cache"] = {}



# File uploader

uploaded_files = st.file_uploader("Upload one or more documents", type=["pdf", "docx", "txt"], accept_multiple_files=True)

if uploaded_files:

Â  Â  all_text = ""

Â  Â  for file in uploaded_files:

Â  Â  Â  Â  if file.type == "application/pdf":

Â  Â  Â  Â  Â  Â  all_text += extract_text_from_pdf(file) + "\n"

Â  Â  Â  Â  elif file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":

Â  Â  Â  Â  Â  Â  all_text += extract_text_from_docx(file) + "\n"

Â  Â  Â  Â  elif file.type == "text/plain":

Â  Â  Â  Â  Â  Â  all_text += extract_text_from_txt(file) + "\n"

Â  Â  st.session_state["documents"] = all_text

Â  Â  st.success("Documents processed successfully!")



# Question input

question = st.text_input("Ask a question about the uploaded documents:")

if question:

Â  Â  if not st.session_state["documents"]:

Â  Â  Â  Â  st.warning("Please upload documents first.")

Â  Â  else:

Â  Â  Â  Â  # Retrieve relevant text using embeddings

Â  Â  Â  Â  document_sentences = st.session_state["documents"].split(". ")

Â  Â  Â  Â  question_embedding = embedding_model.encode(question, convert_to_tensor=True)

Â  Â  Â  Â  doc_embeddings = embedding_model.encode(document_sentences, convert_to_tensor=True)

Â  Â  Â  Â  similarities = util.pytorch_cos_sim(question_embedding, doc_embeddings)[0]

Â  Â  Â  Â  top_sentences = [document_sentences[i] for i in similarities.argsort(descending=True)[:5]]

Â  Â  Â  Â  document_text = " ".join(top_sentences)

Â  Â  Â  Â 

Â  Â  Â  Â  # Get AI response

Â  Â  Â  Â  answer = ask_chatgpt(question, document_text)

Â  Â  Â  Â  st.write("### ðŸ¤– Answer:", answer)
